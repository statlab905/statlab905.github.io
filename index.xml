<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>STAT LAB</title><link>https://statlab905.github.io/</link><atom:link href="https://statlab905.github.io/index.xml" rel="self" type="application/rss+xml"/><description>STAT LAB</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 12 Mar 2025 00:00:00 +0000</lastBuildDate><image><url>https://statlab905.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url><title>STAT LAB</title><link>https://statlab905.github.io/</link></image><item><title>Analysis of Variance of Tensor Product Reproducing Kernel Hilbert Spaces on Metric Spaces</title><link>https://statlab905.github.io/publication/analysis-of-variance-of-tensor-product-reproducing-kernel-hilbert-spaces-on-metric-spaces/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/publication/analysis-of-variance-of-tensor-product-reproducing-kernel-hilbert-spaces-on-metric-spaces/</guid><description/></item><item><title>Seminar on Optimization Algorithms</title><link>https://statlab905.github.io/seminar/2024opt/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/2024opt/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This seminar provides a comprehensive overview of optimization algorithms, covering both theoretical foundations and practical applications. We begin with advanced convex programming techniques, including the Alternating Direction Method with Gaussian Back Substitution and optimally linearized ADMM. The seminar then explores modern optimization methods such as Adam and generalized primal–dual hybrid gradient approaches for saddle point problems. We also discuss communication-efficient statistical estimation and the balanced augmented Lagrangian method. Further topics include the global linear convergence of ADMM with multi-block variables, decentralized consensus optimization via the EXTRA algorithm, and stochastic gradient descent with variance reduction. Finally, we examine the role of optimization in double descent and investigate why overparameterized neural networks can still generalize effectively.&lt;/p>
&lt;h2 id="time-and-location">Time and Location&lt;/h2>
&lt;p>Time: 2:00pm-4:00pm, Thursday, 2024 Spring&lt;/p>
&lt;p>Location: USTC the 5th Teaching Building and Online&lt;/p>
&lt;h2 id="learning-materials">Learning Materials&lt;/h2>
&lt;p>The materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/1_YuJshk_oS3AcFjk7sIMuA?pwd=9uhs" target="_blank" rel="noopener">PPT&lt;/a>.&lt;/p></description></item><item><title>Seminar on Statistical Computing with Python</title><link>https://statlab905.github.io/seminar/python/</link><pubDate>Sun, 28 Jan 2024 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/python/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This seminar offers a comprehensive exploration of statistical computing using Python, starting with fundamental Python programming and progressing into deep learning and optimization techniques. Topics include computer vision, image processing, and natural language processing (NLP), as well as advanced deep learning models. The seminar also introduces reinforcement learning, covering classical algorithms, Deep Q-Learning, and Policy Gradient (PG) methods, along with their practical applications. Additionally, it delves into distributed computing, providing an overview of Ray and its API for parallel deep learning and reinforcement learning with Ray RLlib. This seminar serves as a technical guide for students and researchers seeking to apply Python in statistical computing and AI-driven problem-solving.&lt;/p>
&lt;h2 id="time-and-location">Time and Location&lt;/h2>
&lt;p>2023 Autumn, USTC Management Science Building and Online&lt;/p>
&lt;h2 id="learning-materials">Learning Materials&lt;/h2>
&lt;p>The materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/1MgDt-kpnB0BS28De7N8itg?pwd=4i5c" target="_blank" rel="noopener">Tutorial&lt;/a>, &lt;a href="https://pan.baidu.com/s/1maBVF7PzLIj80ipSJRFftA?pwd=3w6u" target="_blank" rel="noopener">Video materials&lt;/a>.&lt;/p></description></item><item><title>Seminar on Mechanism-Fused Statistical Inference</title><link>https://statlab905.github.io/seminar/mechanistic_stat/</link><pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/mechanistic_stat/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This seminar explores the fusion of mechanistic modeling and statistical inference, presenting a range of methodologies for parameter estimation, curve fitting, and uncertainty quantification. Beginning with an introduction to mechanistic models, it covers nonlinear least squares, trajectory matching, and nonparametric curve fitting with roughness penalties. Advanced topics include gradient matching, generalized profiled estimation, and local polynomial methods. We further examine equation matching techniques and sparse additive models for ordinary differential equations (ODEs). The seminar also delves into Gaussian processes and their applications in ODE modeling, culminating in a discussion on the novel use of physics-informed neural networks (PINNs) for analyzing Argo data.&lt;/p>
&lt;h2 id="time-and-location">Time and Location&lt;/h2>
&lt;p>2022 Autumn, USTC Management Science Building and Online&lt;/p>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;p>Our materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/1SAd9jOigvDPQhYsuPalk4Q?pwd=hky8" target="_blank" rel="noopener">PPT&lt;/a>.&lt;/p></description></item><item><title>Seminar on Deep Learning</title><link>https://statlab905.github.io/seminar/dl/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/dl/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Our seminar progresses from foundational concepts and text analysis to convolutional neural networks (CNNs), emphasizing image classification and optimization techniques. It explores deep learning applications in natural language processing and computer vision (object detection, semantic segmentation), followed by reinforcement learning principles. Specialized topics include methods for graph-based/non-Euclidean data and medical image analysis, providing a concise yet broad technical overview across key domains.&lt;/p>
&lt;h2 id="time-and-location">Time and Location&lt;/h2>
&lt;p>2021 Summer, Online&lt;/p>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;p>Our materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/17Jftvugz9By73c_xegzW1Q?pwd=kj8u" target="_blank" rel="noopener">Video materials&lt;/a>, &lt;a href="https://pan.baidu.com/s/1DsNrDT1ji7LdG2OpLEw_Jg?pwd=qpu6" target="_blank" rel="noopener">PPT&lt;/a>.&lt;/p></description></item><item><title>Seminar on Nonparametric Statistics</title><link>https://statlab905.github.io/seminar/nonparametric/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/nonparametric/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Our seminar consists two parts. In the first part, we cover selected topics in &amp;ldquo;Learning with Kernel&amp;rdquo;, including Loss functions, Regularization, Pattern Recognition, Implementation, Designing Kernels and Kernel Fisher Discriminant. In the second part, we report our research progress in topics of Nonparametric Statistics, including Tree methods, two sample tests on Hyperbolic Space and Change Point Detection.&lt;/p>
&lt;h2 id="time-and-location">Time and Location&lt;/h2>
&lt;p>Time: 7:00pm-9:00pm, Tuesday, 2022 Spring&lt;/p>
&lt;p>Location: USTC Management Science Building 1008 and Online&lt;/p>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;p>The materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/1kwX_9ccrDjvfWM4Rmrzw-Q?pwd=rw8v" target="_blank" rel="noopener">PPT&lt;/a>.&lt;/p></description></item><item><title>Seminar on Optimization and Algorithm</title><link>https://statlab905.github.io/seminar/optimization/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/optimization/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>We cover selected topics in &amp;ldquo;Numerical Optimization&amp;rdquo;, including Line Search Method, Trust Region Method, Conjugate Gradient Method, Quasi-Newton Methods and Simplex Optimization. Besides, in the middle of the term, we listen Prof Bingsheng He from Nanjing University presenting his work on Optimization.&lt;/p>
&lt;h2 id="time-and-location">Time and Location&lt;/h2>
&lt;p>Time: 7:00pm-9:00pm, Tuesday, 2022 Spring&lt;/p>
&lt;p>Location: USTC Management Science Building 1008 and Online&lt;/p>
&lt;h2 id="learning-materials">Learning Materials&lt;/h2>
&lt;p>The materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/17Dq-gcn_ZsZ_XJ1_xWLmtQ?pwd=k5hy" target="_blank" rel="noopener">PPT&lt;/a>.&lt;/p></description></item><item><title>Seminar on Reinforcement Learning</title><link>https://statlab905.github.io/seminar/rl/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/seminar/rl/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Our seminar mainly focuses on the book &amp;ldquo;Reinforcement Learning :An Introduction&amp;rdquo;. We the basic parts of Reinforcement Learning, including Multi Armed Bandit, Thompson Sampling, Markov Decision Process, Monte Carlo Methods, Temporal-Difference Learning, On-policy Approximations and Policy Gradient Methods. We not only present the Theory of the methods in our seminar, but also use some experiment to illustrate our method.&lt;/p>
&lt;h2 id="time">Time&lt;/h2>
&lt;p>9:30am-11:30am, Tuesday, 2022 Spring&lt;/p>
&lt;h2 id="materials">Materials&lt;/h2>
&lt;p>The materials are stored in Baidu Cloud Drive: &lt;a href="https://pan.baidu.com/s/13s5xRZo4Tt-SFNEYMLrh-Q?pwd=9qkj" target="_blank" rel="noopener">PPT&lt;/a>.&lt;/p></description></item><item><title>Seminar on Nonparametric Statistics</title><link>https://statlab905.github.io/event/nonparametric/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/event/nonparametric/</guid><description/></item><item><title>Seminar on Optimization and Algorithm</title><link>https://statlab905.github.io/event/optimization/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/event/optimization/</guid><description>&lt;p>Materials are continuously updated on our &lt;a href="https://github.com/Everglow00/Optimization-and-Algorithms" target="_blank" rel="noopener">Github repository&lt;/a>.&lt;/p></description></item><item><title>Seminar on Reinforcement Learning</title><link>https://statlab905.github.io/event/rl/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/event/rl/</guid><description>&lt;h2 id="time">Time&lt;/h2>
&lt;p>9:30am-11:30am, Tuesday, 2022 Spring&lt;/p>
&lt;h2 id="materials">Materials&lt;/h2>
&lt;p>Materials are continuously updated on our &lt;a href="https://github.com/ljq1492/RL-AL_2022Spring" target="_blank" rel="noopener">Github repository&lt;/a>.&lt;/p></description></item><item><title/><link>https://statlab905.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/admin/config.yml</guid><description/></item><item><title/><link>https://statlab905.github.io/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/contact/</guid><description/></item><item><title/><link>https://statlab905.github.io/people/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/people/</guid><description/></item><item><title>abess -- A Fast Best-Subset Selection Library in Python and R</title><link>https://statlab905.github.io/softwares/abess/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/softwares/abess/</guid><description>&lt;a href="https://anaconda.org/conda-forge/abess">
&lt;img src='https://anaconda.org/conda-forge/abess/badges/platforms.svg' align="left"/>&lt;/a>
&lt;/a>
&lt;a href="https://badge.fury.io/py/abess">
&lt;img src='https://badge.fury.io/py/abess.svg' align="left"/>&lt;/a>
&lt;/a>
&lt;a href="https://anaconda.org/conda-forge/abess">
&lt;img src='https://img.shields.io/conda/vn/conda-forge/abess.svg' align="left"/>&lt;/a>
&lt;/a>
&lt;a href="https://cran.r-project.org/package=abess">
&lt;img src='https://img.shields.io/cran/v/abess?logo=R' align="left"/>&lt;/a>
&lt;/a>
&lt;a href="https://pepy.tech/project/abess">
&lt;img src='https://pepy.tech/badge/abess' align="left"/>&lt;/a>
&lt;/a>
&lt;p>&lt;img src='https://raw.githubusercontent.com/abess-team/abess/master/docs/image/icon_long.png' align="center"/>&lt;/a>&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;code>abess&lt;/code> (Adaptive BEst Subset Selection) library aims to solve general best subset selection, i.e.,
find a small subset of predictors such that the resulting model is expected to have the highest accuracy.
The selection for best subset shows great value in scientific researches and practical applications.
For example, clinicians want to know whether a patient is healthy or not based on the expression levels of a few of important genes.&lt;/p>
&lt;p>This library implements a generic algorithm framework to find the optimal solution in an extremely fast way.
This framework now supports the detection of best subset under:
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/1-glm/plot_1_LinearRegression.html" target="_blank" rel="noopener">linear regression&lt;/a>,
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/1-glm/plot_2_LogisticRegression.html" target="_blank" rel="noopener">classification (binary or multi-class)&lt;/a>,
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/1-glm/plot_5_PossionGammaRegression.html" target="_blank" rel="noopener">counting-response modeling&lt;/a>,
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/1-glm/plot_4_CoxRegression.html#sphx-glr-auto-gallery-1-glm-plot-4-coxregression-py" target="_blank" rel="noopener">censored-response modeling&lt;/a>,
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/1-glm/plot_3_MultiTaskLearning.html" target="_blank" rel="noopener">multi-response modeling (multi-tasks learning)&lt;/a>, etc.
It also supports the variants of best subset selection like
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/3-advanced-features/plot_best_group.html" target="_blank" rel="noopener">group best subset selection&lt;/a>,
&lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/3-advanced-features/plot_best_nuisance.html" target="_blank" rel="noopener">nuisance penalized regression&lt;/a>,
Especially, the time complexity of (group) best subset selection for linear regression is certifiably polynomial.&lt;/p>
&lt;h2 id="quick-start">Quick start&lt;/h2>
&lt;p>The &lt;code>abess&lt;/code> software has both Python and R&amp;rsquo;s interfaces. Here a quick start will be given and for more details, please view: &lt;a href="https://abess.readthedocs.io/en/latest/Installation.html" target="_blank" rel="noopener">Installation&lt;/a>.&lt;/p>
&lt;h3 id="python-package">Python package&lt;/h3>
&lt;p>Install the stable version of Python-package from &lt;a href="https://pypi.org/project/abess/" target="_blank" rel="noopener">Pypi&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">$ pip install abess
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or &lt;a href="https://anaconda.org/conda-forge/abess" target="_blank" rel="noopener">conda-forge&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">$ conda install abess
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Best subset selection for linear regression on a simulated dataset in Python:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">abess.linear&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">LinearRegression&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">abess.datasets&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">make_glm_data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sim_dat&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_glm_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">300&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">family&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;gaussian&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LinearRegression&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sim_dat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sim_dat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>See more examples analyzed with Python in the &lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/index.html" target="_blank" rel="noopener">Python tutorials&lt;/a>.&lt;/p>
&lt;h3 id="r-package">R package&lt;/h3>
&lt;p>Install the stable version of R-package from &lt;a href="https://cran.r-project.org/web/packages/abess" target="_blank" rel="noopener">CRAN&lt;/a> with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">install.packages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;abess&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Best subset selection for linear regression on a simulated dataset in R:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">abess&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sim_dat&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">generate.data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">300&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1000&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">abess&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sim_dat[[&lt;/span>&lt;span class="s">&amp;#34;x&amp;#34;&lt;/span>&lt;span class="n">]]&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sim_dat[[&lt;/span>&lt;span class="s">&amp;#34;y&amp;#34;&lt;/span>&lt;span class="n">]]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>See more examples analyzed with R in the &lt;a href="https://abess-team.github.io/abess/articles/" target="_blank" rel="noopener">R tutorials&lt;/a>.&lt;/p>
&lt;h2 id="runtime-performance">Runtime Performance&lt;/h2>
&lt;p>To show the power of abess in computation, we assess its timings of the CPU execution (seconds) on synthetic datasets, and compare to state-of-the-art variable selection methods. The variable selection and estimation results are deferred to &lt;a href="https://abess.readthedocs.io/en/latest/auto_gallery/1-glm/plot_a1_power_of_abess.html" target="_blank" rel="noopener">Python performance&lt;/a> and &lt;a href="https://abess-team.github.io/abess/articles/v11-power-of-abess.html" target="_blank" rel="noopener">R performance&lt;/a>. All computations are conducted on a Ubuntu platform with Intel(R) Core(TM) i9-9940X CPU @ 3.30GHz and 48 RAM.&lt;/p>
&lt;h3 id="python-package-1">Python package&lt;/h3>
&lt;p>We compare &lt;code>abess&lt;/code> Python package with &lt;code>scikit-learn&lt;/code> on linear regression and logistic regression. Results are presented in the below figure:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/softwares/abess/timings_hufa34008d660dc2da4616bf0df2f4e6a6_18148_31d060037dda3cd81216ccee5ca23e00.webp 400w,
/softwares/abess/timings_hufa34008d660dc2da4616bf0df2f4e6a6_18148_100034ebcb39ade4cf79b48dbfe18e30.webp 760w,
/softwares/abess/timings_hufa34008d660dc2da4616bf0df2f4e6a6_18148_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://statlab905.github.io/softwares/abess/timings_hufa34008d660dc2da4616bf0df2f4e6a6_18148_31d060037dda3cd81216ccee5ca23e00.webp"
width="760"
height="326"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>It can be see that &lt;code>abess&lt;/code> uses the least runtime to find the solution.&lt;/p>
&lt;h3 id="r-package-1">R package&lt;/h3>
&lt;p>We compare &lt;code>abess&lt;/code> R package with three widely used R packages: &lt;code>glmnet&lt;/code>, &lt;code>ncvreg&lt;/code>, and &lt;code>L0Learn&lt;/code>.
We get the runtime comparison results:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/softwares/abess/r_runtime_hu2b29aa87e6659e8df2f59d10e1664348_62285_32bb9ed4c0c7e521f9a39deb939f1aed.webp 400w,
/softwares/abess/r_runtime_hu2b29aa87e6659e8df2f59d10e1664348_62285_93d8e29098844ac79d9a27a9b8be8100.webp 760w,
/softwares/abess/r_runtime_hu2b29aa87e6659e8df2f59d10e1664348_62285_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://statlab905.github.io/softwares/abess/r_runtime_hu2b29aa87e6659e8df2f59d10e1664348_62285_32bb9ed4c0c7e521f9a39deb939f1aed.webp"
width="760"
height="276"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Compared with other packages,
&lt;code>abess&lt;/code> shows competitive computational efficiency,
and achieves the best computational power when variables have a large correlation.&lt;/p>
&lt;h2 id="open-source-software">Open source software&lt;/h2>
&lt;p>&lt;code>abess&lt;/code> is a free software and its source code is publicly available on &lt;a href="https://github.com/abess-team/abess" target="_blank" rel="noopener">Github&lt;/a>. The core framework is programmed in C++, and user-friendly R and Python interfaces are offered. You can redistribute it and/or modify it under the terms of the &lt;a href="https://www.gnu.org/licenses/gpl-3.0.html" target="_blank" rel="noopener">GPL-v3 License&lt;/a>. We welcome contributions for &lt;code>abess&lt;/code>, especially stretching &lt;code>abess&lt;/code> to the other best subset selection problems.&lt;/p>
&lt;h2 id="whats-news">What&amp;rsquo;s news&lt;/h2>
&lt;p>New features:&lt;/p>
&lt;ul>
&lt;li>&lt;code>abess&lt;/code> Python package can be installed via &lt;code>conda&lt;/code>.&lt;/li>
&lt;li>&lt;code>abess&lt;/code> R package is is highlighted as one of the core packages in &lt;a href="https://cran.r-project.org/web/views/MachineLearning.html" target="_blank" rel="noopener">CRAN Task View: Machine Learning &amp;amp; Statistical Learning&lt;/a>.&lt;/li>
&lt;li>On Windows, the recommended C++ compiler shifts from Mingw to Microsoft Visual Studio.&lt;/li>
&lt;li>Support predicting survival function in &lt;code>abess.linear.CoxPHSurvivalAnalysis&lt;/code>.&lt;/li>
&lt;li>Rename estimators in Python. Please check &lt;a href="https://abess.readthedocs.io/en/latest/Python-package/index.html" target="_blank" rel="noopener">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>New best subset selection tasks:&lt;/p>
&lt;ul>
&lt;li>Generalized linear model for ordinal regression (a.k.a rank learning in some machine learning literature).&lt;/li>
&lt;/ul>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>If you use &lt;code>abess&lt;/code> or reference our tutorials in a presentation or publication, we would appreciate citations of our library.&lt;/p>
&lt;blockquote>
&lt;p>Jin Zhu, Xueqin Wang, Liyuan Hu, Junhao Huang, Kangkang Jiang, Yanhang Zhang, Shiyun Lin, Junxian Zhu (2022). “abess: A Fast Best Subset Selection Library in Python and R.” Journal of Machine Learning Research (Accepted).&lt;/p>
&lt;/blockquote>
&lt;p>The corresponding BibteX entry:&lt;/p>
&lt;pre tabindex="0">&lt;code>@article{zhu2022abess,
author = {Jin Zhu and Xueqin Wang and Liyuan Hu and Junhao Huang and Kangkang Jiang and Yanhang Zhang and Shiyun Lin and Junxian Zhu},
title = {abess: A Fast Best Subset Selection Library in Python and R},
journal = {Journal of Machine Learning Research (Accepted)},
year = {2022}
}
&lt;/code>&lt;/pre>&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>Junxian Zhu, Canhong Wen, Jin Zhu, Heping Zhang, and Xueqin Wang (2020). A polynomial algorithm for best-subset selection problem. Proceedings of the National Academy of Sciences, 117(52):33117-33123.&lt;/li>
&lt;li>Pölsterl, S (2020). scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn. J. Mach. Learn. Res., 21(212), 1-6.&lt;/li>
&lt;li>Yanhang Zhang, Junxian Zhu, Jin Zhu, and Xueqin Wang (2021). Certifiably Polynomial Algorithm for Best Group Subset Selection. arXiv preprint arXiv:2104.12576.&lt;/li>
&lt;li>Qiang Sun and Heping Zhang (2020). Targeted Inference Involving High-Dimensional Data Using Nuisance Penalized Regression, Journal of the American Statistical Association, DOI: 10.1080/01621459.2020.1737079.&lt;/li>
&lt;li>Jin Zhu, Xueqin Wang, Liyuan Hu, Junhao Huang, Kangkang Jiang, Yanhang Zhang, Shiyun Lin, Junxian Zhu (2022). abess: A Fast Best Subset Selection Library in Python and R. Journal of Machine Learning Research (Accepted).&lt;/li>
&lt;/ul></description></item><item><title>Tour</title><link>https://statlab905.github.io/tour/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://statlab905.github.io/tour/</guid><description/></item></channel></rss>